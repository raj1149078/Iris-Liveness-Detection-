{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of IRIS_PRINT_ATTACK.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raj1149078/Iris-Liveness-Detection-/blob/main/Liveness%20Detection%20II.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8_LJ00ceJRL"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from joblib import load, dump\n",
        "from scipy.signal import butter, filtfilt, welch\n",
        "from scipy.stats import entropy, skew, kurtosis, moment"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwBNPIbvT5k1"
      },
      "source": [
        "<h2 align=center>CONSTANTS</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URG_l7fygIBU"
      },
      "source": [
        "PATH = '/content/drive/MyDrive/ETPAD.v2/'\n",
        "LIVE = 'LIVE EYE MOVEMENTS/'\n",
        "SAS_I = 'SAS_I EYE MOVEMENTS/'\n",
        "SAS_II = 'SAS_II EYE MOVEMENTS/'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOgwKgy4UCOo"
      },
      "source": [
        "<h2 align=center>FEATURE EXTRACTION</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmuTir715XKF"
      },
      "source": [
        "class FeatureExtraction:\n",
        "\n",
        "    \"\"\"\n",
        "                        ----------- FEATURE EXTRACTION CLASS ------------\n",
        "\n",
        "            Required params:\n",
        "                - path      : Path to the dataset folder\n",
        "                - folder    : Name of the data folder. e.g. 'LIVE EYE MOVEMENTS/',\n",
        "                             'SAS_I EYE MOVEMENTS/' etc.\n",
        "                - label     : For live data label = 1 and for spoof label = 0\n",
        "            Oprional params:\n",
        "                - unit_size : size of a single unit. Must be able to divide 15000 \n",
        "                              with no remainder. \n",
        "                - take_size : 1500 (FIXED FOR THIS DATASET)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, path, folder, label, unit_size = 1500, take_size=15000):\n",
        "        self.path = path\n",
        "        self.unit_size = unit_size\n",
        "        self.folder = folder\n",
        "        self.label = label\n",
        "        self.take_size = take_size\n",
        "\n",
        "    @staticmethod\n",
        "    def get_local_unit_centroid(u, M):\n",
        "        return np.sum(u, axis=1) / M\n",
        "\n",
        "    @staticmethod\n",
        "    def get_local_unit_power(u, M):\n",
        "        return np.sum(u ** 2, axis=1) / M\n",
        "\n",
        "    @staticmethod\n",
        "    def get_local_unit_variance(u, LUC, M):\n",
        "        return np.sum(np.subtract(u, LUC.reshape(-1, 1)) ** 2, axis=1) / M\n",
        "\n",
        "    @staticmethod\n",
        "    def get_local_unit_snr(LUC, LUV):\n",
        "        LUV[LUV == 0] = 0.0000001\n",
        "        return np.divide(LUC, np.sqrt(LUV))\n",
        "\n",
        "    @staticmethod\n",
        "    def get_local_unit_invalidity(validity, M):\n",
        "        return np.sum(validity, axis=1) / M\n",
        "\n",
        "    @staticmethod\n",
        "    def get_entropy(u):\n",
        "        u[u == 0] = 0.0000001\n",
        "        temp = np.multiply(u, np.log(np.abs(u)))\n",
        "        return np.sum(temp, axis=1)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_max_spectral_power(u, fs=1000):\n",
        "        f, p = welch(u, fs=fs, axis=1)\n",
        "        return np.max(p, axis=1)\n",
        "    \n",
        "    #@staticmethod\n",
        "    #def get_skew (u):\n",
        "    #return pskew(axis = 1, bias = True) \n",
        "\n",
        "    @staticmethod\n",
        "    def get_filtered_data(data, cut_off=5, fs=1000):\n",
        "        b, a = butter(N=4, Wn=cut_off, btype='low', fs=fs)\n",
        "        return filtfilt(b, a, data)\n",
        "\n",
        "    def apply_temporal_filter(self, data):\n",
        "        x = data['x'].to_numpy()\n",
        "        y = data['y'].to_numpy()\n",
        "        validity = data['validity'].to_numpy()\n",
        "        x_velocity = np.gradient(x)\n",
        "        y_velocity = np.gradient(y)\n",
        "        filter_condition = (x_velocity < 5) & (y_velocity < 5)\n",
        "        x_filt = x[~filter_condition]\n",
        "        y_filt = y[~filter_condition]\n",
        "        validity_filt = validity[~filter_condition]\n",
        "        extra_elements = x_filt.shape[0] % self.unit_size\n",
        "        reshaped_x = x_filt[:(x_filt.shape[0] - extra_elements)]\n",
        "        reshaped_y = y_filt[:(y_filt.shape[0] - extra_elements)]\n",
        "        reshaped_v = validity_filt[:(validity_filt.shape[0] - extra_elements)]\n",
        "        return reshaped_x.reshape(-1, self.unit_size), \\\n",
        "        reshaped_y.reshape(-1, self.unit_size), \\\n",
        "        reshaped_v.reshape(-1, self.unit_size)\n",
        "\n",
        "    def load_file(self, filename):\n",
        "        data = pd.read_csv(self.path + self.folder + filename, \n",
        "                           delim_whitespace=True, skiprows=1, header=None)\n",
        "        data.columns = ['sample', 'x', 'y', 'validity']\n",
        "        return data\n",
        "\n",
        "    def extract(self):\n",
        "        features = np.empty((1, 39))\n",
        "\n",
        "        # ------ GET ALL FILENAMES IN THE FOLDER ------\n",
        "        file_names = os.listdir(self.path + self.folder)\n",
        "\n",
        "        # -------- ITERATE THROUGH ALL THE FILES ----------- \n",
        "        # for file in file_names:\n",
        "        for file in tqdm(file_names, desc=\"EXTRACTING... \"):\n",
        "            data = self.load_file(file)\n",
        "\n",
        "            # ------------ APPLY FILTER ----------\n",
        "            # x = self.get_filtered_data(data['x'].to_numpy()) \\\n",
        "            #         .reshape(-1, self.unit_size)\n",
        "            # y = self.get_filtered_data(data['y'].to_numpy()) \\\n",
        "            #         .reshape(-1, self.unit_size)\n",
        "\n",
        "            # x = data['x'].to_numpy().reshape(-1, self.unit_size)\n",
        "            # y = data['y'].to_numpy().reshape(-1, self.unit_size)\n",
        "            # validity = data['validity'].to_numpy().reshape(-1, self.unit_size)\n",
        "\n",
        "            x, y, validity = self.apply_temporal_filter(data)\n",
        "\n",
        "            # --------------- UNIT FEATURES --------------\n",
        "            LUCx = self.get_local_unit_centroid(x, self.unit_size)\n",
        "            LUCy = self.get_local_unit_centroid(y, self.unit_size)\n",
        "            LUPx = self.get_local_unit_power(x, self.unit_size)\n",
        "            LUPy = self.get_local_unit_power(y, self.unit_size)\n",
        "            LUVx = self.get_local_unit_variance(x, LUCx, self.unit_size)\n",
        "            LUVy = self.get_local_unit_variance(y, LUCy, self.unit_size)\n",
        "            LUSx = self.get_local_unit_snr(LUCx, LUVx)\n",
        "            LUSy = self.get_local_unit_snr(LUCy, LUVy)\n",
        "            LUI = self.get_local_unit_invalidity(validity, self.unit_size)\n",
        "            ENTx = self.get_entropy(x)\n",
        "            ENTy = self.get_entropy(y)\n",
        "            MSPx = self.get_max_spectral_power(x)\n",
        "            MSPy = self.get_max_spectral_power(y)\n",
        "            skewx = skew(x, axis=1)\n",
        "            skewy = skew(y, axis=1)\n",
        "            kurx = kurtosis(x, axis=1)\n",
        "            kury = kurtosis(y, axis=1)\n",
        "            Mntx = moment (x, axis=1)\n",
        "            Mnty = moment (y, axis=1)\n",
        "\n",
        "            \n",
        "\n",
        "            # --------- COMBINE EVERYTHING --------------\n",
        "            temp = np.stack([LUCx, LUCy, LUPx, LUPy, LUVx, \n",
        "                             LUVy, LUSx, LUSy, LUI, MSPx, MSPy, ENTx, ENTy, skewx, skewy, kurx, kury, Mntx, Mnty], axis=1)\n",
        "            \n",
        "            # ----------- AVERAGE AND STD -------------\n",
        "            f_avg = np.mean(temp, axis=0).reshape(1, -1)\n",
        "            f_std = np.std(temp, axis=0).reshape(1, -1)\n",
        "\n",
        "            # ------ FINAL FEATURE VECTOR INCLUDING LABEL (400x19) ----------\n",
        "            f_final = np.concatenate([f_avg, f_std, self.label], axis=1)\n",
        "            features = np.append(features, f_final, axis=0)\n",
        "        return features[1:, :]\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3tPGQPu6iDZ",
        "outputId": "abc2cf04-cd91-4b97-e466-6704d18827e7"
      },
      "source": [
        "# ------------- EXTRACT FEATURE FOR ALL 3 CLASSES --------------\n",
        "FE = FeatureExtraction(PATH, LIVE, np.array([[1]]))\n",
        "live_features = FE.extract()\n",
        "FE = FeatureExtraction(PATH, SAS_I, np.array([[0]]))\n",
        "sasi_features = FE.extract()\n",
        "FE = FeatureExtraction(PATH, SAS_II, np.array([[0]]))\n",
        "sasii_features = FE.extract()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EXTRACTING... : 100%|██████████| 400/400 [00:07<00:00, 52.31it/s]\n",
            "EXTRACTING... : 100%|██████████| 400/400 [00:07<00:00, 51.35it/s]\n",
            "EXTRACTING... : 100%|██████████| 400/400 [00:08<00:00, 45.08it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A74MTBUH6rJE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e47afbb-f197-4bf1-cd6d-5c099f680f28"
      },
      "source": [
        "# ----------- WRITE FEATURE VECTOR TO DRIVE -----------\n",
        "dump(live_features, PATH + 'LIVE_FEATURES.joblib')\n",
        "dump(sasi_features, PATH + 'SAS_I_FEATURES.joblib')\n",
        "dump(sasii_features, PATH + 'SAS_II_FEATURES.joblib')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/ETPAD.v2/SAS_II_FEATURES.joblib']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvHvYSeoUIr0"
      },
      "source": [
        "<h2 align=center>CLASSIFICATION</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhRMp52f54rR"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, ShuffleSplit\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.decomposition import PCA\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ky2zMWuQ587t"
      },
      "source": [
        "# ------------- READ FEATURES FROM FILE --------------\n",
        "# live_features = load(PATH + 'LIVE_FEATURES.joblib')\n",
        "# sasi_features = load(PATH + 'SAS_I_FEATURES.joblib')\n",
        "# sasii_features = load(PATH + 'SAS_II_FEATURES.joblib')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwgtY5dZ6Pg8"
      },
      "source": [
        "all_features_i = np.concatenate([live_features, sasi_features], axis=0)\n",
        "all_features_i = all_features_i[~np.isnan(all_features_i).any(axis=1)]\n",
        "X = all_features_i[:, :-1]\n",
        "y = all_features_i[:, -1].astype(int)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=10)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bxwzuA4EZF5"
      },
      "source": [
        "pca = PCA()\r\n",
        "X_train = pca.fit_transform(X_train)\r\n",
        "X_test = pca.transform(X_test)\r\n",
        "explained_variance = pca.explained_variance_ratio_\r\n",
        "#print (explained_variance)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJvLq-Q36smd",
        "outputId": "2a95fddb-509f-4d40-e4f0-d8aaf9946929"
      },
      "source": [
        "#clf = make_pipeline(StandardScaler(), SVC(gamma=1, C=100))\n",
        "\n",
        "clf = RandomForestClassifier(random_state=0)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "#print(\"Confusion Matrix:\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "\n",
        "#clf = GradientBoostingClassifier()\n",
        "cv = ShuffleSplit(n_splits=100, test_size=0.5, random_state=0)\n",
        "scores = cross_val_score(clf, X, y, cv=cv) * 100\n",
        "print(\"AVERAGE ACCURACY: %0.1f%% (+/- %0.2f%%)\" % (scores.mean(), scores.std() * 2))\n",
        "print(\"MAXIMUM ACCURACY: %0.1f%%\" %(scores.max()))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[186  11]\n",
            " [ 18 185]]\n",
            "AVERAGE ACCURACY: 94.4% (+/- 2.10%)\n",
            "MAXIMUM ACCURACY: 96.8%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cYfRheKIBi1"
      },
      "source": [
        "all_features_ii = np.concatenate([live_features, sasii_features], axis=0)\r\n",
        "all_features_ii = all_features_ii[~np.isnan(all_features_ii).any(axis=1)]\r\n",
        "X = all_features_ii[:, :-1]\r\n",
        "y = all_features_ii[:, -1].astype(int)\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=10)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiZHdiYKIBOz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e61907c-339f-4803-9e6f-f7c1a3f2a058"
      },
      "source": [
        "#clf = make_pipeline(StandardScaler(), SVC(gamma=2, C=100))\r\n",
        "\r\n",
        "clf = RandomForestClassifier(random_state=0)\r\n",
        "clf.fit(X_train, y_train)\r\n",
        "\r\n",
        "y_pred = clf.predict(X_test)\r\n",
        "\r\n",
        "#Confusion Matrix\r\n",
        "print(\"Confusion Matrix:\")\r\n",
        "cm = confusion_matrix(y_test, y_pred)\r\n",
        "print(cm)\r\n",
        "\r\n",
        "cv = ShuffleSplit(n_splits=100, test_size=0.5, random_state=0)\r\n",
        "scores = cross_val_score(clf, X, y, cv=cv) * 100\r\n",
        "print(\"AVERAGE ACCURACY: %0.1f%% (+/- %0.2f%%)\" % (scores.mean(), scores.std() * 2))\r\n",
        "print(\"MAXIMUM ACCURACY: %0.1f%%\" %(scores.max()))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            "[[190   7]\n",
            " [  8 195]]\n",
            "AVERAGE ACCURACY: 97.0% (+/- 1.76%)\n",
            "MAXIMUM ACCURACY: 99.5%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
